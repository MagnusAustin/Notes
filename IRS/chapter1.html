<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Information Retrieval System</title>
</head>
<body>

<h1>Logical View of a Document</h1>

<h2>Historical Representation</h2>
<p>Documents are often represented by a set of index terms or keywords. These keywords can be extracted directly from the document text or specified by a human expert. These keywords provide a logical view of the document.</p>

<h2>Full Text Logical View</h2>
<p>Modern computers allow for the representation of a document by its full set of words, known as the full text logical view. This approach is computationally expensive, especially with large collections.</p>

<h2>Reduction of Keywords</h2>
<p>For large document collections, the set of representative keywords may need to be reduced. Techniques include eliminating stop words, using stemming, and identifying noun groups to reduce complexity.</p>

<h2>Text Operations</h2>
<p>Text operations or transformations like compression further reduce the document's complexity. These transformations help shift the logical view from full text to a set of index terms.</p>

<h2>Logical View Continuum</h2>
<p>Full text representation offers a complete view but incurs higher computational costs. A small set of categories, generated by a specialist, offers a concise view but might lower retrieval quality. Intermediate logical views can balance between completeness and computational efficiency. The internal structure of a document (e.g., chapters, sections) can be recognized and used in structured text retrieval models.</p>

<h2>Continuum Representation</h2>
<p>The logical view of a document can shift smoothly from full text to a higher-level, human-specified representation. This continuum allows for various levels of document representation in an information retrieval system.</p>

<hr>

<h1>Search Engine Information Retrieval</h1>

<h2>Indexing Process</h2>
<p>The indexing process comprises three main tasks:</p>

<h3>Text Acquisition</h3>
<ul>
    <li><strong>Web Crawlers:</strong> Follow links to find documents.</li>
    <li><strong>Feeds:</strong> Real-time streams of documents (news, blogs, etc.).</li>
    <li><strong>Conversion:</strong> Converts various document formats (HTML, XML, PDF) into a consistent text-plus-metadata format.</li>
    <li><strong>Document Data Store:</strong> Stores text, metadata (e.g., type, creation date), and other related content for fast access.</li>
</ul>

<h3>Text Transformation</h3>
<ul>
    <li><strong>Parsing:</strong> Recognizes structural elements in the text.</li>
    <li><strong>Stopping:</strong> Removes stop words.</li>
    <li><strong>Stemming:</strong> Reduces words to their root form.</li>
    <li><strong>Link Analysis:</strong> Identifies popularity and community information through links.</li>
    <li><strong>Information Extraction:</strong> Identifies important index terms.</li>
    <li><strong>Classifier:</strong> Identifies class-related metadata for documents.</li>
</ul>

<h3>Index Creation</h3>
<ul>
    <li><strong>Document Statistics:</strong> Gathers statistical information about words and documents.</li>
    <li><strong>Weighting:</strong> Determines the relative importance of words.</li>
    <li><strong>Inversion:</strong> Converts document-term information to term-document format for indexing.</li>
    <li><strong>Index Distribution:</strong> Distributes indexes across multiple computers/sites for fast query processing.</li>
</ul>

<h2>Query Process</h2>
<p>The query process comprises three main tasks:</p>

<h3>User Interaction</h3>
<p>Supports creation and refinement of user queries and displays results.</p>

<h3>Ranking</h3>
<ul>
    <li><strong>Scoring:</strong> Calculates scores for documents using ranking algorithms.</li>
    <li><strong>Optimization:</strong> Designs ranking algorithms for efficient processing.</li>
    <li><strong>Distribution:</strong> Processes queries in a distributed environment.</li>
</ul>

<h3>Evaluation</h3>
<ul>
    <li><strong>Logging:</strong> Records user queries and interactions to improve search effectiveness and efficiency.</li>
    <li><strong>Ranking Analysis:</strong> Measures and tunes ranking effectiveness.</li>
    <li><strong>Performance Analysis:</strong> Measures and tunes system efficiency, including response time and throughput.</li>
</ul>

<hr>

<h1>Objectives of an IRS</h1>

<h2>Primary Objective</h2>
<p>Minimize the user’s time spent locating needed information. Overhead includes time spent on query generation, query execution, scanning results, and reading non-relevant items.</p>

<h2>Success of an Information System</h2>
<p>Success is subjective and varies based on:</p>
<ul>
    <li><strong>Information Needs:</strong> The completeness of the required information.</li>
    <li><strong>User's Willingness:</strong> The user’s acceptance of overhead.</li>
</ul>
<p>Examples include a financial advisor requiring all relevant information for thorough decision-making, and a student needing just enough references to meet academic expectations.</p>

<h2>Retrieval System Features</h2>
<p>Reasonable retrieval requires fewer features and provides sufficient information without overwhelming the user. Comprehensive retrieval can be negative as it may overload the user with too much information, leading to higher overhead in processing irrelevant, albeit relevant, data.</p>

<h2>Relevance in Information Retrieval</h2>
<p>From the user’s perspective, "relevant" and "needed" information are synonymous. From the system’s perspective, information may be relevant to the search query but not necessarily needed by the user.</p>

<h2>Measures of Information Systems</h2>
<p>Precision and Recall are key metrics used to evaluate the effectiveness of a search:</p>
<ul>
    <li><strong>Precision:</strong> The ratio of relevant items retrieved to the total number of items retrieved.</li>
    <li><strong>Recall:</strong> The ratio of relevant items retrieved to the total number of relevant items in the database.</li>
</ul>

<hr>

<h1>Types of IRS Models</h1>

<h2>Types of Information Retrieval (IR) Models</h2>
<p>Information Retrieval (IR) models can be categorized into three main types:</p>

<h3>Classical IR Model</h3>
<ul>
    <li>Based on fundamental mathematical concepts; most widely used.</li>
    <li>Easy to implement.</li>
    <li>Examples: Vector-space, Boolean, and Probabilistic IR models.</li>
</ul>

<h3>Non-Classical IR Model</h3>
<ul>
    <li>Built upon propositional logic.</li>
    <li>Examples: Information Logic, Situation Theory, and Interaction models.</li>
</ul>

<h3>Alternative IR Model</h3>
<ul>
    <li>Enhances classical IR principles to create more functional models.</li>
    <li>Examples: Cluster Model, Alternative Set-Theoretic Models (e.g., Fuzzy Set model), Latent Semantic Indexing (LSI) model, and Alternative Algebraic Models (e.g., Generalized Vector Space Model).</li>
</ul>

<h2>Similarity-Based Classical IR Models</h2>

<h3>Boolean Model</h3>
<p>Information is translated into Boolean expressions and queries. Boolean operations (AND, OR, NOT) are used to create combinations based on user queries. The system determines relevant information based on the truth of the Boolean expression.</p>

<h3>Vector Space Model (VSM)</h3>
<p>Documents and queries are represented as vectors. Retrieval is based on the similarity of these vectors. Vectors can be binary (used in Boolean VSM) or weighted (used in Non-binary VSM).</p>

<h3>Probability Distribution Model</h3>
<p>Documents are seen as distributions of terms. Matching is based on the similarity of these term distributions. Methods include using entropy or computing probable utility. Types include Similarity-based Probability Distribution Model and Expected-utility-based Probability Distribution Model.</p>

<h3>Probabilistic Models</h3>
<p>Documents are ranked based on the probability of their relevance to the query. The system utilizes probability ranking to display results.</p>

<hr>

<h1>Data Retrieval vs Information Retrieval</h1>

<h2>Information Retrieval (IR) vs. Data Retrieval</h2>

<h3>Data Retrieval</h3>
<ul>
    <li>Determines which documents contain the keywords in the user query.</li>
    <li>Often insufficient to satisfy the user's information need.</li>
    <li>Aims to retrieve all objects that satisfy clearly defined conditions (e.g., regular expressions, relational algebra).</li>
    <li>Deals with well-defined and structured data (e.g., in relational databases).</li>
    <li>Provides a solution to retrieving specific data but does not address retrieving comprehensive information about a subject.</li>
</ul>

<h3>Information Retrieval (IR)</h3>
<ul>
    <li>Concerned with retrieving information about a subject, not just matching data to a query.</li>
    <li>Involves extracting syntactic and semantic information from documents and ranking them based on relevance to the user query.</li>
    <li>The primary goal is to minimize the user's effort in finding the required information.</li>
    <li>Works with unstructured or semi-structured data (e.g., natural language text).</li>
    <li>Provides solutions for retrieving a range of information that can fulfill a user’s need, even if they cannot express their query precisely.</li>
</ul>

<h2>Query Construction and Retrieval Models</h2>
<p>Both Data Retrieval and Information Retrieval processes require some level of query construction, but the complexity and nature of these queries differ significantly. Information Retrieval systems often need to deal with vague or incomplete user queries and rely on ranking algorithms to present the most relevant information to the user.</p>

<hr>

</body>
</html>
